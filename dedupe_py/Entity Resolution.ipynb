{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entity Resolution\n",
    "=========\n",
    "\n",
    "This provides an overview of why and how Entity Resolution is becoming an important \n",
    "discipline in [Computer Science and Data Science](http://www.datacommunitydc.org/blog/2013/08/entity-resolution-for-big-data). This notebook explores why we need entity resolution and how to do it. Brief explanations\n",
    "are given regarding commerical options. Open source options are also discussed. Some of these open\n",
    "source options are demonstrated using Python.\n",
    "\n",
    "## Why do we need entity resolution?\n",
    "\n",
    "When doing a statistical analysis, you need to first identify your units of analysis. \n",
    "Borrowing from Allison ([Multiple Regression: A Primer](http://www.amazon.com/Multiple-Regression-Research-Methods-Statistics/dp/0761985336), p. 7):\n",
    "\n",
    "> To do a regression analysis, you first need a set of cases (also called units of analysis or observations).\n",
    "> In the social sciences, the cases are most often persons, but they could also be organizations, countries, or\n",
    "> other groups. In economics, the cases are sometimes units of time, like years or quarters. For each case, you\n",
    "> need measurements on all of the variables in the regession equation. \n",
    "\n",
    "Often, our data does not come to us in one table; ready for analysis. In this situation, we also need to be\n",
    "aware of database concepts.In web development, we can say that we're concerned with defining the database \n",
    "model (see the Python web framework Django for [their model specifications](https://docs.djangoproject.com/en/1.8/topics/db/models/)). In the case of Entity Resolution, we are concerned with two concepts:\n",
    "\n",
    "1. Primary Keys\n",
    "1. Foreign Keys\n",
    "\n",
    "In Django, each model requires exactly one field to have a primary key. A foreign key specifies a one-to-many \n",
    "relationship in Django. They have separate field types for one-to-one and many-to-many relationships between models. For our purposes we will exclude many-to-many relationships and include one-to-one relationships in our discussion of the Foreign key. The following is a demonstration from the [Django Documenation of the Foreign key](https://docs.djangoproject.com/en/1.8/ref/models/fields/#django.db.models.ForeignKey). \n",
    "\n",
    "``` Python\n",
    "from django.db import models\n",
    "\n",
    "class Car(models.Model):\n",
    "    manufacturer = models.ForeignKey('Manufacturer')\n",
    "    # ...\n",
    "\n",
    "class Manufacturer(models.Model):\n",
    "    # ...\n",
    "    pass\n",
    "```\n",
    "\n",
    "We can see that each type of car could have multiple manufacturers. Several car manufacturers make mid-size sedans. Those manufacturers are reflected in the above Django model. The database models are not specific to Python but it does help to have concrete examples with easily accessible documentation.\n",
    "\n",
    "When conducting a statistical analysis, we want to run our model on one table. Before conducting the analysis, we must be sure of the following:\n",
    "\n",
    "1. Each unit of analysis occurs only once \n",
    "  * Primary keys must be present and unique\n",
    "1. Each unit of analysis includes variables that have been correctly mapped from their original tables\n",
    "  * Foreign keys must be present\n",
    "\n",
    "Entity Resolution is a tool to define primary and foreign keys when these relationships are not previously well defined. For example, let's say people start using the web application described above. They populate the models with cars and\n",
    "the manufacturers who produce them. The developers did not include standard names for cars and manufacturers because\n",
    "they were unsure of what the future may hold. As the data scientist arrives on scene, they find that people have spelled\n",
    "the manufacturer \"GMC\" in a variety of ways like \"General Motors Corporation\" and \"General Motors\". People have also \n",
    "mispelled the manufacturer name. Actually, they have done this across much of the manufacturer and car information. \n",
    "\n",
    "Given the state of this manufacturer/car data, the data scientist is no longer able to ascertain their unit of analysis. They must recreate primary and foreign keys before they can perform their statistical analysis. \n",
    "\n",
    "  \n",
    "**TL;DR** We can't begin our analysis unless we have defined and identified quantifiable units of analysis. Data often shows up in multiple tables, we need to know about primary and foreign keys. We want to run our statistical analysis on a single table. We must determine primary and foreign keys before we can manipulate our data to perform the statistical analysis. Entity resolution can be used to establish primary and foreign keys when those relationships are not previously well defined.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## How can we accomplish entity resolution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have established that entity resolution is a way for the data scientist to restablish primary and foreign keys once people have inputted information which makes those relationships ambiguous from a statistical analysis standpoint. Here is the same goal restated by the [Stanford Entity Resolution Framework (SERF)](http://infolab.stanford.edu/serf/)Project.\n",
    "\n",
    "> The goal of the SERF project is to develop a generic infrastructure for Entity Resolution (ER). ER (also known as\n",
    "> deduplication, or record linkage) is an important information integration problem: The same \"real-world entities\"\n",
    "> (e.g., customers, or products) are referred to in different ways in multiple data records. For instance, two records\n",
    "> on the same person may provide different name spellings, and addresses may differ. The goal of ER is to \"resolve\"\n",
    "> entities, by identifying the records that represent the same entity and reconciling them to obtain one record per\n",
    "> entity.\n",
    "\n",
    "You can accomplish entity resolution by:\n",
    "\n",
    "1. Hiring a company\n",
    "1. Doing it yourself\n",
    "1. A little of both\n",
    "\n",
    "We will explore each option. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hire a company\n",
    "\n",
    "[Basis Technology](http://www.basistech.com/) has a product called the [Rosette Entity Resolver](http://www.basistech.com/text-analytics/rosette/entity-resolver/). Their technology has been used\n",
    "by \"Amazon.com, EMC, Endeca/Oracle, Exalead/Dassault, Fujitsu, Google, Hewlett-Packard, Microsoft, Oracle, and governments around the world\". If you're a newer smaller company then you might want to check out their [startup program](http://www.basistech.com/about/startup/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do it yourself\n",
    "\n",
    "There are number of open source tools and frameworks that I've found. None of them are an Apache projects. Here's what I've found:\n",
    "\n",
    "1. [Dedupe](https://github.com/datamade/dedupe) python package\n",
    "1. [Duke](https://github.com/larsga/Duke)\n",
    "1. [elasticsearch-entity-resolution](https://github.com/YannBrrd/elasticsearch-entity-resolution)\n",
    "1. [Berkeley Entity Resolution](https://github.com/gregdurrett/berkeley-entity)\n",
    "1. [SERF Project](http://infolab.stanford.edu/serf/)\n",
    "1. [Ch. 3 of Mining Massive Datasets](http://infolab.stanford.edu/~ullman/mmds/ch3.pdf)\n",
    "\n",
    "Of these options, Dedupe is the only one maintained by an organization ([Datamade](http://datamade.us/)). My personal experience with Duke is that it seemed to have some buzz but none of the demo examples worked for me. The elasticsearch entity resolution plugin is based on Duke (haven't personally tried it). The Berkeley ER project looks like it's Greg's thesis and appears to be a work in progress. When I contacted one of the researchers at the SERF project, they said it's no longer active. Ch.3 of Mining massive datasets included an entity resolution example that may work for certain use cases.\n",
    "\n",
    "Let's take a closer look at *Dedupe* and Ch.3 of Mining Massive datasets (hereafter referred to as *MMDS*). See **appendix 1 & 2** for examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do some of it yourself but hire a company\n",
    "\n",
    "One approach is to refurbish your dataset by kicking the complexity to some else's API. Depending on your\n",
    "data, you may be able to take advantage of the [Google Places API](https://developers.google.com/places/). This would allow you to return standard names for locations and organizations; you may even be able to cross-reference their phone numbers. This API is considerably different than the one produced by Facebook or Foursquare because it's not user-generated; from what I can tell. See **Appendix 3** for an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix 0: Getting Data\n",
    "\n",
    "Our data is from the Dedupe [test datasets](https://github.com/datamade/dedupe/tree/master/tests/datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns are identical: [ True  True  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "restaurant_1 = pd.read_csv(\"datasets/restaurant-1.csv\")\n",
    "restaurant_2 = pd.read_csv(\"datasets/restaurant-2.csv\")\n",
    "\n",
    "print(\"Columns are identical: {}\".format(restaurant_1.columns == restaurant_2.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "      <th>city</th>\n",
       "      <th>cuisine</th>\n",
       "      <th>unique_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>112</td>\n",
       "      <td>112</td>\n",
       "      <td>112</td>\n",
       "      <td>112</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>112</td>\n",
       "      <td>111</td>\n",
       "      <td>16</td>\n",
       "      <td>31</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>mesa grill</td>\n",
       "      <td>\"3434 peachtree rd. ne\"</td>\n",
       "      <td>\"new york city\"</td>\n",
       "      <td>\"american (new)\"</td>\n",
       "      <td>'12'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>43</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              name                   address              city  \\\n",
       "count          112                       112               112   \n",
       "unique         112                       111                16   \n",
       "top     mesa grill   \"3434 peachtree rd. ne\"   \"new york city\"   \n",
       "freq             1                         2                43   \n",
       "\n",
       "                  cuisine unique_id  \n",
       "count                 112       112  \n",
       "unique                 31       112  \n",
       "top      \"american (new)\"      '12'  \n",
       "freq                   20         1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restaurant_1.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix 1: MMDS Approach\n",
    "\n",
    "The main moving part here is the edit distance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix 2: Dedupe (Python Package)\n",
    "\n",
    "Note that [dedupe tests](https://github.com/datamade/dedupe/tree/master/tests) includes the referenced datasets. I've modified the tests so they play well within a notebook. We will use Dedupe to solve the following problems:\n",
    "\n",
    "\n",
    "1. Deduplication\n",
    "1. Record linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import csv\n",
    "import datetime\n",
    "import multiprocessing\n",
    "from itertools import combinations\n",
    "from future.utils import viewitems\n",
    "\n",
    "import dedupe\n",
    "import exampleIO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deduplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# filepath\n",
    "settings_file = 'canonical_learned_settings.json'\n",
    "raw_data = 'datasets/restaurant-nophone-training.csv'\n",
    "\n",
    "# params\n",
    "fields  = [{'field' : 'name', 'type': 'String'},\n",
    "                      {'field' : 'name', 'type': 'Exact'},\n",
    "                      {'field' : 'address', 'type': 'String'},\n",
    "                      {'field' : 'cuisine', 'type': 'ShortString'},\n",
    "                      {'field' : 'city', 'type' : 'ShortString'}\n",
    "                      ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def canonicalImport(filename):\n",
    "    \"\"\" Clean and import data \"\"\"\n",
    "    preProcess = exampleIO.preProcess\n",
    "\n",
    "    data_d = {}\n",
    "\n",
    "    with open(filename) as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for (i, row) in enumerate(reader):\n",
    "            clean_row = [(k, preProcess(v)) for (k, v) in\n",
    "                         viewitems(row)]\n",
    "            data_d[i] = dedupe.core.frozendict(clean_row)\n",
    "\n",
    "    return data_d, reader.fieldnames\n",
    "\n",
    "def evaluateDuplicates(found_dupes, true_dupes):\n",
    "    \"\"\" Log information about duplicates that are identified \"\"\"\n",
    "    true_positives = found_dupes.intersection(true_dupes)\n",
    "    false_positives = found_dupes.difference(true_dupes)\n",
    "    uncovered_dupes = true_dupes.difference(found_dupes)\n",
    "\n",
    "    print('found duplicate')\n",
    "    print('duplicate length: {}'.format(len(found_dupes)))\n",
    "\n",
    "    print('precision: {}'.format(1 - len(false_positives) \\\n",
    "                                        / float(len(found_dupes))))\n",
    "\n",
    "    print('recall: {}'.format(len(true_positives) \\\n",
    "                                     / float(len(true_dupes))))\n",
    "    \n",
    "    \n",
    "    \n",
    "def handleSettingsFile(settings_file, cores, sample_size, \n",
    "                       training_pairs, fields):\n",
    "    \"\"\" Create settings file if it doesn't exist \n",
    "    \n",
    "    Args:\n",
    "        settings_file: str, path to settings file (json)\n",
    "        cores:  int, multiprocessing.cpu_count()\n",
    "        sample_size: int, # of samples\n",
    "        training_pairs: dedupe.trainingDataDedupe()\n",
    "        fields: list of dicts, field specifications for deduping\n",
    "        \n",
    "    Returns:\n",
    "        void, settings file updated or written to disk\n",
    "    \n",
    "    \"\"\"\n",
    "    if os.path.exists(settings_file):\n",
    "        with open(settings_file, 'rb') as f:\n",
    "            deduper = dedupe.StaticDedupe(f, 1)\n",
    "            f.close()\n",
    "        return deduper\n",
    "    \n",
    "    else:\n",
    "        fields = fields\n",
    "\n",
    "        deduper = dedupe.Dedupe(fields, num_cores=cores)\n",
    "        deduper.sample(data_d, sample_size)\n",
    "        deduper.markPairs(training_pairs)\n",
    "        deduper.train()\n",
    "        with open(settings_file, 'wb') as f:\n",
    "            deduper.writeSettings(f)\n",
    "            f.close()\n",
    "        return deduper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of known duplicate pairs 112\n",
      "clustering...\n",
      "Evaluate Clustering\n",
      "found duplicate\n",
      "duplicate length: 115\n",
      "precision: 0.9391304347826087\n",
      "recall: 0.9642857142857143\n",
      "ran in  0.37821149826049805 seconds\n"
     ]
    }
   ],
   "source": [
    "## run the deduplication\n",
    "\n",
    "\n",
    "data_d, header = canonicalImport(raw_data)\n",
    "\n",
    "training_pairs = dedupe.trainingDataDedupe(data_d, \n",
    "                                           'unique_id', \n",
    "                                            5000)\n",
    "\n",
    "deduper = handleSettingsFile(settings_file = settings_file, \n",
    "                       cores = multiprocessing.cpu_count(),\n",
    "                       sample_size = 10000,\n",
    "                       training_pairs = training_pairs,\n",
    "                       fields = fields)\n",
    "\n",
    "\n",
    "duplicates_s = set(frozenset(pair) for pair in training_pairs['match'])\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "print('number of known duplicate pairs', len(duplicates_s))\n",
    "\n",
    "alpha = deduper.threshold(data_d, 1.5)\n",
    "\n",
    "# print candidates\n",
    "print('clustering...')\n",
    "clustered_dupes = deduper.match(data_d, threshold=alpha)\n",
    "\n",
    "print('Evaluate Clustering')\n",
    "confirm_dupes = set([])\n",
    "for dupes, score in clustered_dupes:\n",
    "    for pair in combinations(dupes, 2):\n",
    "        confirm_dupes.add(frozenset((data_d[pair[0]], \n",
    "                                     data_d[pair[1]])))\n",
    "\n",
    "evaluateDuplicates(confirm_dupes, duplicates_s)\n",
    "\n",
    "print('ran in ', time.time() - t0, 'seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# format output into a table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix 3: Google Places API\n",
    "\n",
    "Please verify with Google that this is not a violation of their terms of service before implementing this approach.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
