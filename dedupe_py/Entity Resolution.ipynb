{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entity Resolution\n",
    "=========\n",
    "\n",
    "This provides an overview of why and how Entity Resolution is becoming an important \n",
    "discipline in [Computer Science and Data Science](http://www.datacommunitydc.org/blog/2013/08/entity-resolution-for-big-data). This notebook explores why we need entity resolution and how to do it. Brief explanations\n",
    "are given regarding commerical options. Open source options are also discussed. Some of these open\n",
    "source options are demonstrated using Python.\n",
    "\n",
    "## Why do we need entity resolution?\n",
    "\n",
    "When doing a statistical analysis, you need to first identify your units of analysis. \n",
    "Borrowing from Allison ([Multiple Regression: A Primer](http://www.amazon.com/Multiple-Regression-Research-Methods-Statistics/dp/0761985336), p. 7):\n",
    "\n",
    "> To do a regression analysis, you first need a set of cases (also called units of analysis or observations).\n",
    "> In the social sciences, the cases are most often persons, but they could also be organizations, countries, or\n",
    "> other groups. In economics, the cases are sometimes units of time, like years or quarters. For each case, you\n",
    "> need measurements on all of the variables in the regession equation. \n",
    "\n",
    "Often, our data does not come to us in one table; ready for analysis. In this situation, we also need to be\n",
    "aware of database concepts.In web development, we can say that we're concerned with defining the database \n",
    "model (see the Python web framework Django for [their model specifications](https://docs.djangoproject.com/en/1.8/topics/db/models/)). In the case of Entity Resolution, we are concerned with two concepts:\n",
    "\n",
    "1. Primary Keys\n",
    "1. Foreign Keys\n",
    "\n",
    "In Django, each model requires exactly one field to have a primary key. A foreign key specifies a one-to-many \n",
    "relationship in Django. They have separate field types for one-to-one and many-to-many relationships between models. For our purposes we will exclude many-to-many relationships and include one-to-one relationships in our discussion of the Foreign key. The following is a demonstration from the [Django Documenation of the Foreign key](https://docs.djangoproject.com/en/1.8/ref/models/fields/#django.db.models.ForeignKey). \n",
    "\n",
    "``` Python\n",
    "from django.db import models\n",
    "\n",
    "class Car(models.Model):\n",
    "    manufacturer = models.ForeignKey('Manufacturer')\n",
    "    # ...\n",
    "\n",
    "class Manufacturer(models.Model):\n",
    "    # ...\n",
    "    pass\n",
    "```\n",
    "\n",
    "We can see that each type of car could have multiple manufacturers. Several car manufacturers make mid-size sedans. Those manufacturers are reflected in the above Django model. The database models are not specific to Python but it does help to have concrete examples with easily accessible documentation.\n",
    "\n",
    "When conducting a statistical analysis, we want to run our model on one table. Before conducting the analysis, we must be sure of the following:\n",
    "\n",
    "1. Each unit of analysis occurs only once \n",
    "1. Each unit of analysis includes variables that have been correctly mapped from their original tables\n",
    "1. Primary and Foreign keys must be present and unique\n",
    "\n",
    "Entity Resolution is a tool to define primary and foreign keys when these relationships are not previously well defined. For example, let's say people start using the web application described above. They populate the models with cars and\n",
    "the manufacturers who produce them. The developers did not include standard names for cars and manufacturers because\n",
    "they were unsure of what the future may hold. As the data scientist arrives on scene, they find that people have spelled\n",
    "the manufacturer \"GMC\" in a variety of ways like \"General Motors Corporation\" and \"General Motors\". People have also \n",
    "mispelled the manufacturer name. Actually, they have done this across much of the manufacturer and car information. \n",
    "\n",
    "Given the state of this manufacturer/car data, the data scientist is no longer able to ascertain their unit of analysis. They must recreate primary and foreign keys before they can perform their statistical analysis. \n",
    "\n",
    "  \n",
    "**TL;DR** We can't begin our analysis unless we have defined and identified quantifiable units of analysis. Data often shows up in multiple tables, we need to know about primary and foreign keys. We want to run our statistical analysis on a single table. We must determine primary and foreign keys before we can manipulate our data to perform the statistical analysis. Entity resolution can be used to establish primary and foreign keys when those relationships are not previously well defined.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## How can we accomplish entity resolution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have established that entity resolution is a way for the data scientist to restablish primary and foreign keys once people have inputted information which makes those relationships ambiguous from a statistical analysis standpoint. Here is the same goal restated by the [Stanford Entity Resolution Framework (SERF)](http://infolab.stanford.edu/serf/)Project.\n",
    "\n",
    "> The goal of the SERF project is to develop a generic infrastructure for Entity Resolution (ER). ER (also known as\n",
    "> deduplication, or record linkage) is an important information integration problem: The same \"real-world entities\"\n",
    "> (e.g., customers, or products) are referred to in different ways in multiple data records. For instance, two records\n",
    "> on the same person may provide different name spellings, and addresses may differ. The goal of ER is to \"resolve\"\n",
    "> entities, by identifying the records that represent the same entity and reconciling them to obtain one record per\n",
    "> entity.\n",
    "\n",
    "You can accomplish entity resolution by:\n",
    "\n",
    "1. Hiring a company\n",
    "1. Doing it yourself\n",
    "1. A little of both\n",
    "\n",
    "We will explore each option. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hire a company\n",
    "\n",
    "[Basis Technology](http://www.basistech.com/) has a product called the [Rosette Entity Resolver](http://www.basistech.com/text-analytics/rosette/entity-resolver/). Their technology has been used\n",
    "by \"Amazon.com, EMC, Endeca/Oracle, Exalead/Dassault, Fujitsu, Google, Hewlett-Packard, Microsoft, Oracle, and governments around the world\". If you're a newer smaller company then you might want to check out their [startup program](http://www.basistech.com/about/startup/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do it yourself\n",
    "\n",
    "There are number of open source tools and frameworks that I've found. None of them are an Apache projects. Here's what I've found:\n",
    "\n",
    "1. [Dedupe](https://github.com/datamade/dedupe) python package\n",
    "1. [Duke](https://github.com/larsga/Duke)\n",
    "1. [elasticsearch-entity-resolution](https://github.com/YannBrrd/elasticsearch-entity-resolution)\n",
    "1. [Berkeley Entity Resolution](https://github.com/gregdurrett/berkeley-entity)\n",
    "1. [SERF Project](http://infolab.stanford.edu/serf/)\n",
    "1. [Ch. 3 of Mining Massive Datasets](http://infolab.stanford.edu/~ullman/mmds/ch3.pdf)\n",
    "\n",
    "Of these options, Dedupe is the only one maintained by an organization ([Datamade](http://datamade.us/)). My personal experience with Duke is that it seemed to have some buzz but none of the demo examples worked for me. The elasticsearch entity resolution plugin is based on Duke (haven't personally tried it). The Berkeley ER project looks like it's Greg's thesis and appears to be a work in progress. When I contacted one of the researchers at the SERF project, they said it's no longer active. Ch.3 of Mining massive datasets included an entity resolution example that may work for certain use cases.\n",
    "\n",
    "Let's take a closer look at *Dedupe* and Ch.3 of Mining Massive datasets (hereafter referred to as *MMDS*). See **appendix 1 & 2** for examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do some of it yourself but hire a company\n",
    "\n",
    "One approach is to refurbish your dataset by kicking the complexity to some else's API. Depending on your\n",
    "data, you may be able to take advantage of the [Google Places API](https://developers.google.com/places/). This would allow you to return standard names for locations and organizations; you may even be able to cross-reference their phone numbers. This API is considerably different than the one produced by Facebook or Foursquare because it's not user-generated; from what I can tell. See **Appendix 3** for an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix 0: Getting Data\n",
    "\n",
    "Our data is from the Dedupe [examples](https://github.com/datamade/dedupe-examples)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix 1: MMDS Approach\n",
    "\n",
    "The main moving part here is the edit distance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import edit_distance\n",
    "\n",
    "edit_distance('foo', 'far')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix 2: Dedupe (Python Package)\n",
    "\n",
    "Note that [dedupe tests](https://github.com/datamade/dedupe/tree/master/tests) includes the referenced datasets. I've modified the tests so they play well within a notebook. We will use Dedupe to solve the following problems:\n",
    "\n",
    "\n",
    "1. Deduplication\n",
    "1. Record linkage\n",
    "\n",
    "See [dedupe examples](https://github.com/datamade/dedupe-examples) for examples that include mysql and postgres. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deduplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example that I've demonstrated below are taken from these scripts. I've already trained the data; so you can just run this notebook and see results. I had to run the example outside of the notebook because `TypeError` is raised when a cell with an empty value is encountered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.system(\"python csv_example/csv_example.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that this is example data, we already know which duplicates exist in the\n",
    "date. Let's evaluate our procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluating output\n",
    "\n",
    "manual_clusters = 'csv_example/csv_example_input_with_true_ids.csv'\n",
    "dedupe_clusters = 'csv_example/csv_example_output.csv'\n",
    "\n",
    "from csv_example.csv_evaluation import dupePairs, evaluateDuplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found duplicate\n",
      "5634\n",
      "precision\n",
      "0.9774582889598864\n",
      "recall\n",
      "0.8333837772397095\n"
     ]
    }
   ],
   "source": [
    "true_dupes = dupePairs(manual_clusters, 'True Id')\n",
    "test_dupes = dupePairs(dedupe_clusters, 'Cluster ID')\n",
    "\n",
    "evaluateDuplicates(test_dupes, true_dupes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dedupe_output = pd.read_csv(dedupe_clusters)\n",
    "input_file = 'csv_example/csv_example_messy_input.csv'\n",
    "dedupe_input = pd.read_csv(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Site name</th>\n",
       "      <th>Address</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Salvation Army - Temple / Salvation Army</td>\n",
       "      <td>1 N Ogden Ave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Salvation Army - Temple / Salvation Army</td>\n",
       "      <td>1 N Ogden Ave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>National Louis University - Dr. Effie O. Elli...</td>\n",
       "      <td>10 S Kedzie Ave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>National Louis University - Dr. Effie O. Elli...</td>\n",
       "      <td>10 S Kedzie Ave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Board Trustees-City Colleges of Chicago - Oli...</td>\n",
       "      <td>10001 S Woodlawn Ave</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id                                          Site name  \\\n",
       "0   0           Salvation Army - Temple / Salvation Army   \n",
       "1   1           Salvation Army - Temple / Salvation Army   \n",
       "2   2   National Louis University - Dr. Effie O. Elli...   \n",
       "3   3   National Louis University - Dr. Effie O. Elli...   \n",
       "4   4   Board Trustees-City Colleges of Chicago - Oli...   \n",
       "\n",
       "                 Address  \n",
       "0         1 N Ogden Ave   \n",
       "1         1 N Ogden Ave   \n",
       "2       10 S Kedzie Ave   \n",
       "3       10 S Kedzie Ave   \n",
       "4  10001 S Woodlawn Ave   "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dedupe_input[[\"Id\", \"Site name\", \"Address\"]][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cluster ID</th>\n",
       "      <th>confidence_score</th>\n",
       "      <th>Id</th>\n",
       "      <th>Site name</th>\n",
       "      <th>Address</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18</td>\n",
       "      <td>0.831860</td>\n",
       "      <td>0</td>\n",
       "      <td>Salvation Army - Temple / Salvation Army</td>\n",
       "      <td>1 N Ogden Ave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>0.831860</td>\n",
       "      <td>1</td>\n",
       "      <td>Salvation Army - Temple / Salvation Army</td>\n",
       "      <td>1 N Ogden Ave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>390</td>\n",
       "      <td>0.547965</td>\n",
       "      <td>2</td>\n",
       "      <td>National Louis University - Dr. Effie O. Elli...</td>\n",
       "      <td>10 S Kedzie Ave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>390</td>\n",
       "      <td>0.547965</td>\n",
       "      <td>3</td>\n",
       "      <td>National Louis University - Dr. Effie O. Elli...</td>\n",
       "      <td>10 S Kedzie Ave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>591</td>\n",
       "      <td>0.438202</td>\n",
       "      <td>4</td>\n",
       "      <td>Board Trustees-City Colleges of Chicago - Oli...</td>\n",
       "      <td>10001 S Woodlawn Ave</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Cluster ID  confidence_score  Id  \\\n",
       "0          18          0.831860   0   \n",
       "1          18          0.831860   1   \n",
       "2         390          0.547965   2   \n",
       "3         390          0.547965   3   \n",
       "4         591          0.438202   4   \n",
       "\n",
       "                                           Site name                Address  \n",
       "0           Salvation Army - Temple / Salvation Army         1 N Ogden Ave   \n",
       "1           Salvation Army - Temple / Salvation Army         1 N Ogden Ave   \n",
       "2   National Louis University - Dr. Effie O. Elli...       10 S Kedzie Ave   \n",
       "3   National Louis University - Dr. Effie O. Elli...       10 S Kedzie Ave   \n",
       "4   Board Trustees-City Colleges of Chicago - Oli...  10001 S Woodlawn Ave   "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dedupe_output[[\"Cluster ID\", \"confidence_score\", \"Id\", \"Site name\", \"Address\"]][0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record linkage\n",
    "\n",
    "This example is also taken from the dedupe example scripts. I had to make some small modifications for it to run but I'll try to submit those as a pull request. I've already trained the data so you can just run this notebook and see the results.\n",
    "\n",
    "From the example script docstring:\n",
    "\n",
    "> This code demonstrates how to use RecordLink with two comma separated\n",
    "> values (CSV) files. We have listings of products from two different\n",
    "> online stores. The task is to link products between the datasets.\n",
    "\n",
    "> The output will be a CSV with our linkded results.\n",
    "\n",
    "I've pulled out the interesting bits of the example script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from future.builtins import next\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "import collections\n",
    "#import logging\n",
    "import optparse\n",
    "import numpy\n",
    "\n",
    "import dedupe\n",
    "from unidecode import unidecode\n",
    "\n",
    "# from example script\n",
    "from record_linkage_example.record_linkage import (preProcess, readData, \n",
    "                                                   descriptions)\n",
    "\n",
    "# params\n",
    "\n",
    "output_file = 'record_linkage_example/data_matching_output.csv'\n",
    "settings_file = 'record_linkage_example/data_matching_learned_settings'\n",
    "training_file = 'record_linkage_example/data_matching_training.json'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing data ...\n"
     ]
    }
   ],
   "source": [
    "print('importing data ...')\n",
    "data_1 = readData('record_linkage_example/AbtBuy_Abt.csv')\n",
    "data_2 = readData('record_linkage_example/AbtBuy_Buy.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading from record_linkage_example/data_matching_learned_settings\n"
     ]
    }
   ],
   "source": [
    "# we already know that the settings file exists\n",
    "if os.path.exists(settings_file):\n",
    "    print('reading from', settings_file)\n",
    "    with open(settings_file, 'rb') as sf :\n",
    "        linker = dedupe.StaticRecordLink(sf)\n",
    "        \n",
    "else:\n",
    "    raise TypeError(\"Notebook is not meant to be run outside example\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clustering...\n",
      "# duplicate sets 359\n"
     ]
    }
   ],
   "source": [
    "# ## Blocking\n",
    "\n",
    "# ## Clustering\n",
    "\n",
    "# Find the threshold that will maximize a weighted average of our\n",
    "# precision and recall.  When we set the recall weight to 2, we are\n",
    "# saying we care twice as much about recall as we do precision.\n",
    "#\n",
    "# If we had more data, we would not pass in all the blocked data into\n",
    "# this function but a representative sample.\n",
    "\n",
    "print('clustering...')\n",
    "linked_records = linker.match(data_1, data_2, 0)\n",
    "\n",
    "print('# duplicate sets', len(linked_records))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ## Writing Results\n",
    "\n",
    "# Write our original data back out to a CSV with a new column called \n",
    "# 'Cluster ID' which indicates which records refer to each other.\n",
    "\n",
    "cluster_membership = {}\n",
    "cluster_id = None\n",
    "for cluster_id, (cluster, score) in enumerate(linked_records):\n",
    "    for record_id in cluster:\n",
    "        cluster_membership[record_id] = (cluster_id, score)\n",
    "\n",
    "if cluster_id :\n",
    "    unique_id = cluster_id + 1\n",
    "else :\n",
    "    unique_id =0\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(output_file, 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    \n",
    "    header_unwritten = True\n",
    "\n",
    "    for fileno, filename in enumerate(('record_linkage_example/AbtBuy_Abt.csv', \n",
    "                                       'record_linkage_example/AbtBuy_Buy.csv')) :\n",
    "        with open(filename) as f_input :\n",
    "            reader = csv.reader(f_input)\n",
    "\n",
    "            if header_unwritten :\n",
    "                heading_row = next(reader)\n",
    "                heading_row.insert(0, 'source file')\n",
    "                heading_row.insert(0, 'Link Score')\n",
    "                heading_row.insert(0, 'Cluster ID')\n",
    "                writer.writerow(heading_row)\n",
    "                header_unwritten = False\n",
    "            else :\n",
    "                next(reader)\n",
    "\n",
    "            for row_id, row in enumerate(reader):\n",
    "                cluster_details = cluster_membership.get(filename + str(row_id))\n",
    "                if cluster_details is None :\n",
    "                    cluster_id = unique_id\n",
    "                    unique_id += 1\n",
    "                    score = None\n",
    "                else :\n",
    "                    cluster_id, score = cluster_details\n",
    "                row.insert(0, fileno)\n",
    "                row.insert(0, score)\n",
    "                row.insert(0, cluster_id)\n",
    "                writer.writerow(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Linksys EtherFast 8-Port 10/100 Switch - EZXS88W</td>\n",
       "      <td>Linksys EtherFast 8-Port 10/100 Switch - EZXS8...</td>\n",
       "      <td>$44.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Linksys EtherFast10/100 5-Port Auto-Sensing Sw...</td>\n",
       "      <td>Linksys EtherFast10/100 5-Port Auto-Sensing Sw...</td>\n",
       "      <td>$29.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Netgear ProSafe 5 Port 10/100 Desktop Switch -...</td>\n",
       "      <td>Netgear ProSafe 5 Port 10/100 Desktop Switch -...</td>\n",
       "      <td>$40.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Belkin F3H982-10 Pro Series High Integrity 10 ...</td>\n",
       "      <td>Belkin F3H982-10 Pro Series High Integrity 10 ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Netgear Prosafe 16 Port 10/100 Rackmount Switc...</td>\n",
       "      <td>Netgear Prosafe 16 Port 10/100 Rackmount Switc...</td>\n",
       "      <td>$131.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   unique_id                                              title  \\\n",
       "0          1   Linksys EtherFast 8-Port 10/100 Switch - EZXS88W   \n",
       "1          2  Linksys EtherFast10/100 5-Port Auto-Sensing Sw...   \n",
       "2          3  Netgear ProSafe 5 Port 10/100 Desktop Switch -...   \n",
       "3          4  Belkin F3H982-10 Pro Series High Integrity 10 ...   \n",
       "4          5  Netgear Prosafe 16 Port 10/100 Rackmount Switc...   \n",
       "\n",
       "                                         description    price  \n",
       "0  Linksys EtherFast 8-Port 10/100 Switch - EZXS8...   $44.00  \n",
       "1  Linksys EtherFast10/100 5-Port Auto-Sensing Sw...   $29.00  \n",
       "2  Netgear ProSafe 5 Port 10/100 Desktop Switch -...   $40.00  \n",
       "3  Belkin F3H982-10 Pro Series High Integrity 10 ...      NaN  \n",
       "4  Netgear Prosafe 16 Port 10/100 Rackmount Switc...  $131.00  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record_input1 = pd.read_csv('record_linkage_example/AbtBuy_Abt.csv')\n",
    "record_input2 = pd.read_csv('record_linkage_example/AbtBuy_Buy.csv')\n",
    "record_output = pd.read_csv('record_linkage_example/data_matching_output.csv')\n",
    "\n",
    "record_input1[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Linksys EtherFast EZXS88W Ethernet Switch - EZ...</td>\n",
       "      <td>Linksys EtherFast 8-Port 10/100 Switch (New/Wo...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Linksys EtherFast EZXS55W Ethernet Switch</td>\n",
       "      <td>5 x 10/100Base-TX LAN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Netgear ProSafe FS105 Ethernet Switch - FS105NA</td>\n",
       "      <td>NETGEAR FS105 Prosafe 5 Port 10/100 Desktop Sw...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Belkin Pro Series High Integrity VGA/SVGA Moni...</td>\n",
       "      <td>1 x HD-15 - 1 x HD-15 - 10ft - Beige</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Netgear ProSafe JFS516 Ethernet Switch</td>\n",
       "      <td>Netgear ProSafe 16 Port 10/100 Rackmount Switc...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   unique_id                                              title  \\\n",
       "0          1  Linksys EtherFast EZXS88W Ethernet Switch - EZ...   \n",
       "1          2          Linksys EtherFast EZXS55W Ethernet Switch   \n",
       "2          3    Netgear ProSafe FS105 Ethernet Switch - FS105NA   \n",
       "3          4  Belkin Pro Series High Integrity VGA/SVGA Moni...   \n",
       "4          5             Netgear ProSafe JFS516 Ethernet Switch   \n",
       "\n",
       "                                         description price  \n",
       "0  Linksys EtherFast 8-Port 10/100 Switch (New/Wo...   NaN  \n",
       "1                              5 x 10/100Base-TX LAN   NaN  \n",
       "2  NETGEAR FS105 Prosafe 5 Port 10/100 Desktop Sw...   NaN  \n",
       "3               1 x HD-15 - 1 x HD-15 - 10ft - Beige   NaN  \n",
       "4  Netgear ProSafe 16 Port 10/100 Rackmount Switc...   NaN  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record_input2[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cluster ID</th>\n",
       "      <th>Link Score</th>\n",
       "      <th>source file</th>\n",
       "      <th>unique_id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>189</td>\n",
       "      <td>0.999620</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Linksys EtherFast 8-Port 10/100 Switch - EZXS88W</td>\n",
       "      <td>Linksys EtherFast 8-Port 10/100 Switch - EZXS8...</td>\n",
       "      <td>$44.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>359</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Linksys EtherFast10/100 5-Port Auto-Sensing Sw...</td>\n",
       "      <td>Linksys EtherFast10/100 5-Port Auto-Sensing Sw...</td>\n",
       "      <td>$29.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>360</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Netgear ProSafe 5 Port 10/100 Desktop Switch -...</td>\n",
       "      <td>Netgear ProSafe 5 Port 10/100 Desktop Switch -...</td>\n",
       "      <td>$40.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>208</td>\n",
       "      <td>0.998993</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>Belkin F3H982-10 Pro Series High Integrity 10 ...</td>\n",
       "      <td>Belkin F3H982-10 Pro Series High Integrity 10 ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>361</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>Netgear Prosafe 16 Port 10/100 Rackmount Switc...</td>\n",
       "      <td>Netgear Prosafe 16 Port 10/100 Rackmount Switc...</td>\n",
       "      <td>$131.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Cluster ID  Link Score  source file  unique_id  \\\n",
       "0         189    0.999620            0          1   \n",
       "1         359         NaN            0          2   \n",
       "2         360         NaN            0          3   \n",
       "3         208    0.998993            0          4   \n",
       "4         361         NaN            0          5   \n",
       "\n",
       "                                               title  \\\n",
       "0   Linksys EtherFast 8-Port 10/100 Switch - EZXS88W   \n",
       "1  Linksys EtherFast10/100 5-Port Auto-Sensing Sw...   \n",
       "2  Netgear ProSafe 5 Port 10/100 Desktop Switch -...   \n",
       "3  Belkin F3H982-10 Pro Series High Integrity 10 ...   \n",
       "4  Netgear Prosafe 16 Port 10/100 Rackmount Switc...   \n",
       "\n",
       "                                         description    price  \n",
       "0  Linksys EtherFast 8-Port 10/100 Switch - EZXS8...   $44.00  \n",
       "1  Linksys EtherFast10/100 5-Port Auto-Sensing Sw...   $29.00  \n",
       "2  Netgear ProSafe 5 Port 10/100 Desktop Switch -...   $40.00  \n",
       "3  Belkin F3H982-10 Pro Series High Integrity 10 ...      NaN  \n",
       "4  Netgear Prosafe 16 Port 10/100 Rackmount Switc...  $131.00  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record_output[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not as obvious what the record linkage procedure is doing. Given that we're working with example data, \n",
    "unique id's were already present so we can see which one's are correct. We can see in the next sell that the\n",
    "unique id's match up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record_input1.count()['unique_id'] + record_input2.count()['unique_id'] \\\n",
    "== record_output.count()['unique_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix 3: Google Places API\n",
    "\n",
    "Please verify with Google that this is not a violation of their terms of service before implementing this approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
